{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "tX[tX < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import sample_data, standardize\n",
    "seed = 1\n",
    "\n",
    "tX_mean = np.nanmean(tX,axis=0)\n",
    "\n",
    "tX_std = np.nanstd(tX,axis=0)\n",
    "norm_tX = np.subtract(tX, tX_mean, where=np.isfinite(tX_mean))\n",
    "norm_tX = np.divide(norm_tX, tX_std, where=tX_std>0)\n",
    "norm_tX[np.isnan(norm_tX)] =0\n",
    "#norm_tX = np.c_[np.ones(num_samples), norm_tX]\n",
    "\n",
    "#y, norm_tX = sample_data(y, norm_tX, seed, len(y[0]))#size_samples= 100000)\n",
    "norm_tX, mean_x, std_x = standardize(norm_tX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "    #return np.exp(-np.logaddexp(0, -t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    \n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    \n",
    "    grad = tx.T.dot(pred - y)\n",
    "    #print('le gradient')\n",
    "    #print(grad)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    #print(loss)\n",
    "    return np.squeeze(-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    #print(loss)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w -= gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.00000009\n",
    "    losses = []\n",
    "    sign = True\n",
    "    #print('on affiche losses')\n",
    "    #print(np.shape(losses))\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    #w = least_squares(y, tx)\n",
    "    y = np.expand_dims(y, axis=1)\n",
    "    \n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        \n",
    "        \n",
    "       \n",
    "            \n",
    "            \n",
    "        if(loss < 20000):\n",
    "            if(loss < 7000):\n",
    "                gamma = 0.0000000005\n",
    "                if(loss < 1000):\n",
    "                    gamma = 0.0000000001\n",
    "                    if(loss < 100):\n",
    "                        gamma = 0.00000000001\n",
    "                        if(loss < 40):\n",
    "                            gamma = 0.000000000009\n",
    "                        \n",
    "                        \n",
    "            else:\n",
    "                gamma = 0.000000001\n",
    "        \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}, gamma={g}\".format(i=iter, l=loss, g=gamma))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w))) \n",
    "    \n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79513997526, gamma=9e-08\n",
      "Current iteration=100, loss=12086.40984498414, gamma=1e-09\n",
      "Current iteration=200, loss=8052.144342288651, gamma=1e-09\n",
      "Current iteration=300, loss=5496.583422731463, gamma=5e-10\n",
      "Current iteration=400, loss=3514.0307696425443, gamma=5e-10\n",
      "Current iteration=500, loss=1543.5061312556354, gamma=5e-10\n",
      "Current iteration=600, loss=695.9132725131931, gamma=1e-10\n",
      "Current iteration=700, loss=304.2555216173496, gamma=1e-10\n",
      "Current iteration=800, loss=74.9618041679496, gamma=1e-11\n",
      "Current iteration=900, loss=36.20130909176078, gamma=9e-12\n",
      "loss=1.0040284043061547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights = logistic_regression_gradient_descent_demo(y, norm_tX)\n",
    "\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    print('hessian')\n",
    "    print(np.shape(y))\n",
    "    print(np.shape(pred))\n",
    "    print(np.shape(w))\n",
    "    \n",
    "    pred =np.diag(pred.T[0]) \n",
    "    r = np.multiply(pred, (1-pred))\n",
    "    \n",
    "    return tx.T.dot(r).dot(tx)\n",
    "\n",
    "#w=least_squares(y, norm_tX)\n",
    "#calculate_hessian(y, norm_tX, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    #print(np.shape(w))\n",
    "    #print(np.shape(tx))\n",
    "    #print(np.shape(y))\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    \n",
    "    hessian = calculate_hessian(y, tx, w)\n",
    "    return loss, gradient, hessian\n",
    "\n",
    "#w=least_squares(y,norm_tX)\n",
    "#logistic_regression(y, norm_tX, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.#\n",
    "    \"\"\"\n",
    "    loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "    w -= np.linalg.solve(hessian, gradient)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    print(np.shape(y))\n",
    "    #y = np.expand_dims(y, axis=1)\n",
    "    print(np.shape(w))\n",
    "    print(np.shape(tx))\n",
    "    print(np.shape(y))\n",
    "    \n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\")\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "\n",
    "#logistic_regression_newton_method_demo(y, norm_tX) hessian matrice trop grande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares and Polynomial regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    \n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import compute_mse\n",
    "from plots import *\n",
    "\n",
    "def polynomial_regression(x):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1,3,7,12]\n",
    "    \n",
    "    \n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form dataset to do polynomial regression.\n",
    "        tx = build_poly(x, degree)\n",
    "        \n",
    "        # least squares\n",
    "        weights = least_squares(y, tx)\n",
    "        \n",
    "        \n",
    "        # compute RMSE\n",
    "        rmse = np.sqrt(2 * compute_mse(y, tx, weights))\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "       \n",
    "        # print\n",
    "        #print(np.shape(y))\n",
    "        #print(np.shape(tx))\n",
    "        #print(np.shape(weights))\n",
    "        \n",
    "    return weights\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#weights = polynomial_regression(norm_tX)\n",
    "#weights = polynomial_regression(norm_tX2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    weight = least_squares(y_tr, tx_tr)\n",
    "    \n",
    "    #if ((ratio == 0,9) and (degree == 7)):\n",
    "    weights = weight\n",
    "    #print(weights)\n",
    "\n",
    "    # calculate RMSE for train and test data.\n",
    "    rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo_2(x, y, degree, ratio, seed, w):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    weight = w\n",
    "    \n",
    "    \n",
    "\n",
    "    # calculate RMSE for train and test data.\n",
    "    rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    return weight\n",
    "    \n",
    "    #if ((ratio == 0,9) and (degree == 7)):\n",
    "    \n",
    "    #print(weights)\n",
    "\n",
    "    # calculate RMSE for train and test data.\n",
    "    rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [7]\n",
    "split_ratios = [0.9]\n",
    "#weights = train_test_split_demo_2(norm_tX, y, degree, split_ratio, seed, weights)\n",
    "#weights = train_test_split_demo(norm_tX, y, degree, split_ratio, seed)\n",
    "#print(weights)\n",
    "\n",
    "#for split_ratio in split_ratios:\n",
    "    #for degree in degrees:\n",
    "      #  train_test_split_demo(norm_tX, y, degree, split_ratio, seed)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "#tX_test[tX_test < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_mean = np.nanmean(tX_test,axis=0)\n",
    "tX_test_std = np.nanstd(tX_test,axis=0)\n",
    "norm_tX_test = np.subtract(tX_test, tX_test_mean, where=np.isfinite(tX_test_mean))\n",
    "norm_tX_test = np.divide(norm_tX_test, tX_test_std, where=tX_test_std>0)\n",
    "norm_tX_test = np.c_[np.ones((y.shape[0], 1)), norm_tX_test]\n",
    "#norm_tX_test = build_poly(norm_tX_test, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.18512471e-01]\n",
      " [ 7.54498170e-03]\n",
      " [-2.31343383e-01]\n",
      " [-7.36392543e-03]\n",
      " [ 1.00090770e-01]\n",
      " [ 1.10259604e-01]\n",
      " [ 1.02141798e-01]\n",
      " [-9.52617251e-02]\n",
      " [ 3.43406399e-02]\n",
      " [-2.48680395e-02]\n",
      " [ 7.10424253e-02]\n",
      " [-1.27639651e-01]\n",
      " [ 1.65819248e-01]\n",
      " [ 1.06507895e-01]\n",
      " [ 1.44645602e-01]\n",
      " [-8.68444579e-04]\n",
      " [-3.22895969e-03]\n",
      " [-2.31962345e-02]\n",
      " [ 5.67026833e-04]\n",
      " [ 2.89662835e-03]\n",
      " [-4.37065458e-03]\n",
      " [ 4.60402877e-03]\n",
      " [ 5.90859737e-02]\n",
      " [ 6.28703060e-02]\n",
      " [ 3.36388848e-02]\n",
      " [ 1.03863673e-04]\n",
      " [-4.23042045e-05]\n",
      " [-2.42682110e-02]\n",
      " [ 6.29620467e-04]\n",
      " [-2.08265797e-03]\n",
      " [ 5.60120526e-02]]\n",
      "[[-1.62337415]\n",
      " [-1.14891405]\n",
      " [-1.15359958]\n",
      " ...\n",
      " [-0.68263614]\n",
      " [-0.30230101]\n",
      " [-1.5155485 ]]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = 'data/sample-submission' # TODO: fill in desired name of output file for submission\n",
    "#print(np.shape(y))\n",
    "#print(np.shape(norm_tX_test))\n",
    "#print(np.shape(weights))\n",
    "print(weights)\n",
    "y_pred = predict_labels(weights, norm_tX_test)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
