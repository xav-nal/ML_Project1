{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "tX[tX < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 33)\n"
     ]
    }
   ],
   "source": [
    "# split feature 22 by value as planned\n",
    "tX = np.hstack((np.delete(tX, 22,axis=1), np.stack([tX[:,22] == 0, tX[:,22] == 1, tX[:,22] == 2, tX[:,22] == 3]).T))\n",
    "print(np.shape(tX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import sample_data, standardize\n",
    "seed = 1\n",
    "\n",
    "tX_mean = np.nanmean(tX,axis=0)\n",
    "\n",
    "tX_std = np.nanstd(tX,axis=0)\n",
    "norm_tX = np.subtract(tX, tX_mean, where=np.isfinite(tX_mean))\n",
    "norm_tX = np.divide(norm_tX, tX_std, where=tX_std>0)\n",
    "norm_tX[np.isnan(norm_tX)] =0\n",
    "\n",
    "\n",
    "\n",
    "norm_tX = np.c_[np.ones((y.shape[0], 1)), norm_tX]\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "\n",
    "#print(initial_w)\n",
    "\n",
    "print(y)\n",
    "\n",
    "y[y==-1] = 0\n",
    "        \n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Implementation import sigmoid\n",
    "\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 34)\n",
      "(34, 1)\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "Current iteration=0, loss=173286.79513997794, gamma=1e-05\n",
      "Current iteration=100, loss=173272.9512896622, gamma=1e-05\n",
      "Current iteration=200, loss=173259.22002903727, gamma=1e-05\n",
      "Current iteration=300, loss=173245.60055404122, gamma=1e-05\n",
      "Current iteration=400, loss=173232.09206760937, gamma=1e-05\n",
      "Current iteration=500, loss=173218.69377961237, gamma=1e-05\n",
      "Current iteration=600, loss=173205.40490678584, gamma=1e-05\n",
      "Current iteration=700, loss=173192.2246726617, gamma=1e-05\n",
      "Current iteration=800, loss=173179.15230749932, gamma=1e-05\n",
      "Current iteration=900, loss=173166.1870482183, gamma=1e-05\n",
      "Current iteration=1000, loss=173153.32813833142, gamma=1e-05\n",
      "Current iteration=1100, loss=173140.5748278781, gamma=1e-05\n",
      "Current iteration=1200, loss=173127.9263733594, gamma=1e-05\n",
      "Current iteration=1300, loss=173115.38203767297, gamma=1e-05\n",
      "Current iteration=1400, loss=173102.9410900489, gamma=1e-05\n",
      "Current iteration=1500, loss=173090.60280598645, gamma=1e-05\n",
      "Current iteration=1600, loss=173078.3664671913, gamma=1e-05\n",
      "Current iteration=1700, loss=173066.23136151343, gamma=1e-05\n",
      "Current iteration=1800, loss=173054.19678288582, gamma=1e-05\n",
      "Current iteration=1900, loss=173042.2620312637, gamma=1e-05\n",
      "Current iteration=2000, loss=173030.42641256435, gamma=1e-05\n",
      "Current iteration=2100, loss=173018.68923860785, gamma=1e-05\n",
      "Current iteration=2200, loss=173007.04982705833, gamma=1e-05\n",
      "Current iteration=2300, loss=172995.50750136544, gamma=1e-05\n",
      "Current iteration=2400, loss=172984.06159070748, gamma=1e-05\n",
      "Current iteration=2500, loss=172972.71142993378, gamma=1e-05\n",
      "Current iteration=2600, loss=172961.45635950885, gamma=1e-05\n",
      "Current iteration=2700, loss=172950.29572545647, gamma=1e-05\n",
      "Current iteration=2800, loss=172939.22887930475, gamma=1e-05\n",
      "Current iteration=2900, loss=172928.25517803163, gamma=1e-05\n",
      "Current iteration=3000, loss=172917.37398401057, gamma=1e-05\n",
      "Current iteration=3100, loss=172906.5846649579, gamma=1e-05\n",
      "Current iteration=3200, loss=172895.88659387932, gamma=1e-05\n",
      "Current iteration=3300, loss=172885.2791490183, gamma=1e-05\n",
      "Current iteration=3400, loss=172874.7617138041, gamma=1e-05\n",
      "Current iteration=3500, loss=172864.33367680066, gamma=1e-05\n",
      "Current iteration=3600, loss=172853.99443165626, gamma=1e-05\n",
      "Current iteration=3700, loss=172843.7433770534, gamma=1e-05\n",
      "Current iteration=3800, loss=172833.5799166594, gamma=1e-05\n",
      "Current iteration=3900, loss=172823.50345907747, gamma=1e-05\n",
      "loss=172813.5134177981\n",
      "[[ 1.34362703e-02]\n",
      " [ 1.85672892e-04]\n",
      " [-6.38878717e-03]\n",
      " [-2.42917686e-04]\n",
      " [ 3.29591051e-03]\n",
      " [ 1.85172324e-03]\n",
      " [ 1.76455399e-03]\n",
      " [-1.65259864e-03]\n",
      " [ 4.08605078e-04]\n",
      " [-3.83420675e-04]\n",
      " [ 2.56376334e-03]\n",
      " [-3.55705788e-03]\n",
      " [ 4.83848215e-03]\n",
      " [ 1.73910041e-03]\n",
      " [ 4.20129133e-03]\n",
      " [-1.95572528e-05]\n",
      " [-8.29492100e-05]\n",
      " [-6.12787579e-04]\n",
      " [ 2.32570677e-05]\n",
      " [ 7.63452575e-05]\n",
      " [ 2.66634196e-04]\n",
      " [ 1.33690599e-04]\n",
      " [ 2.23752897e-03]\n",
      " [ 1.11447575e-03]\n",
      " [ 3.19649933e-06]\n",
      " [-2.12047832e-06]\n",
      " [-1.76170250e-04]\n",
      " [ 7.28439772e-06]\n",
      " [-3.46256595e-05]\n",
      " [ 2.20377592e-03]\n",
      " [-2.56361289e-03]\n",
      " [ 3.55284733e-04]\n",
      " [ 3.14678671e-03]\n",
      " [-6.01219900e-04]]\n"
     ]
    }
   ],
   "source": [
    "from Implementation import logistic_regression_gd\n",
    "\n",
    "# init parameters\n",
    "max_iter = 4000\n",
    "gamma = 0.00001\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(norm_tX))\n",
    "print(np.shape(initial_w))\n",
    "print(y)\n",
    "\n",
    "\n",
    "    \n",
    "weights = logistic_regression_gd(y, norm_tX, initial_w, max_iter, gamma)\n",
    "\n",
    "print(weights)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79514050332\n",
      "Current iteration=100, loss=454796.8530979841\n",
      "Current iteration=200, loss=471127.5292027239\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-9fe5da9a69df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregu_logistic_regression_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_tX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mregu_logistic_regression_gd\u001b[1;34m(y, tx, lambda_, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_penalized_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mlearning_by_penalized_gd\u001b[1;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m    123\u001b[0m     \u001b[0mDo\u001b[0m \u001b[0mone\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mdescent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpenalized\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m     \"\"\"\n\u001b[0;32m    126\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m#loss = y.T.dot(np.log(y_pred)) + (1 - y).T.dot(np.log(1 - y_pred))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mxw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_i\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Implementation import regu_logistic_regression_gd\n",
    "\n",
    "# init parameters\n",
    "max_iter = 1600\n",
    "gamma = 0.00005\n",
    "#gamma = 0.00000009\n",
    "lambda_ = 0.1\n",
    "\n",
    "\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "\n",
    "weights = regu_logistic_regression_gd(y, norm_tX, lambda_, initial_w, max_iter, gamma )\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares and Polynomial regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = polynomial_regression(norm_tX)\n",
    "#weights = polynomial_regression(norm_tX2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Implementation import compute_mse, calculate_loss\n",
    "\n",
    "def train_test_split_demo(x, y, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    #tx_tr = build_poly(x_tr, degree)\n",
    "    #tx_te = build_poly(x_te, degree)\n",
    "   \n",
    "    tx_tr =x_tr\n",
    "    tx_te = x_te\n",
    "    #tx_tr = np.ones((len(x_tr), 1))\n",
    "    #tx_tr = np.c_[tx_tr, np.power(x_tr, 1)]\n",
    "    #tx_te = np.ones((len(x_te), 1))\n",
    "    #tx_te = np.c_[tx_te, np.power(x_te, 1)]\n",
    "    \n",
    "\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    gamma = 0.00000009\n",
    "    initial_w = np.zeros((tx_tr.shape[1], 1))\n",
    "    \n",
    "    print(np.shape(y_tr))\n",
    "    print(np.shape(tx_tr))\n",
    "    print(np.shape(initial_w))\n",
    "    \n",
    "\n",
    "\n",
    "    weight = logistic_regression_gd(y_tr, tx_tr, initial_w, max_iter, gamma)\n",
    "    \n",
    "    y_tr = np.expand_dims(y_tr, axis=1)\n",
    "    y_te = np.expand_dims(y_te, axis=1)\n",
    "    \n",
    "    # calculate RMSE for train and test data.\n",
    "    #rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    #rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "    \n",
    "    #calculate cost for train and test data\n",
    "    cost_tr = calculate_loss(y_tr, tx_tr, weight)\n",
    "    cost_te = calculate_loss(y_te, tx_te, weight)\n",
    "\n",
    "    print(\"proportion={p}, logistic reg, Training loss={tr:.3f}, Testing loss={te:.3f}\".format(\n",
    "          p=ratio, tr=cost_tr, te=cost_te))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175000,)\n",
      "(175000, 34)\n",
      "(34, 1)\n",
      "Current iteration=0, loss=121300.75659798345, gamma=9e-08\n",
      "Current iteration=100, loss=16567.575101411116, gamma=1e-09\n",
      "Current iteration=200, loss=14463.579866057495, gamma=1e-09\n",
      "Current iteration=300, loss=12379.821750837247, gamma=1e-09\n",
      "Current iteration=400, loss=10315.788958539182, gamma=1e-09\n",
      "Current iteration=500, loss=8270.987557714107, gamma=1e-09\n",
      "Current iteration=600, loss=6608.270007891391, gamma=5e-10\n",
      "Current iteration=700, loss=5600.506474584108, gamma=5e-10\n",
      "Current iteration=800, loss=4597.279974828474, gamma=5e-10\n",
      "Current iteration=900, loss=3598.535037165144, gamma=5e-10\n",
      "loss=2604.217129898301\n",
      "proportion=0.7, logistic reg, Training loss=2604.217, Testing loss=1366.612\n",
      "(200000,)\n",
      "(200000, 34)\n",
      "(34, 1)\n",
      "Current iteration=0, loss=138629.43611198076, gamma=9e-08\n",
      "Current iteration=100, loss=13434.861866862237, gamma=1e-09\n",
      "Current iteration=200, loss=10760.04721236047, gamma=1e-09\n",
      "Current iteration=300, loss=8113.413455710848, gamma=1e-09\n",
      "Current iteration=400, loss=6224.850634394505, gamma=5e-10\n",
      "Current iteration=500, loss=4921.560889424218, gamma=5e-10\n",
      "Current iteration=600, loss=3624.8866577106237, gamma=5e-10\n",
      "Current iteration=700, loss=2334.736398551846, gamma=5e-10\n",
      "Current iteration=800, loss=1051.0203252600768, gamma=5e-10\n",
      "Current iteration=900, loss=258.3125023670582, gamma=3e-10\n",
      "loss=69.95432615450409\n",
      "proportion=0.8, logistic reg, Training loss=69.954, Testing loss=8.314\n",
      "(225000,)\n",
      "(225000, 34)\n",
      "(34, 1)\n",
      "Current iteration=0, loss=155958.115625978, gamma=9e-08\n",
      "Current iteration=100, loss=13963.525459847166, gamma=1e-09\n",
      "Current iteration=200, loss=10604.567565927602, gamma=1e-09\n",
      "Current iteration=300, loss=7284.980964864226, gamma=1e-09\n",
      "Current iteration=400, loss=5475.56282614237, gamma=5e-10\n",
      "Current iteration=500, loss=3840.484886628532, gamma=5e-10\n",
      "Current iteration=600, loss=2214.6311610323028, gamma=5e-10\n",
      "Current iteration=700, loss=746.2279676356848, gamma=3e-10\n",
      "Current iteration=800, loss=78.53003459256433, gamma=1e-11\n",
      "Current iteration=900, loss=46.34421564999502, gamma=1e-11\n",
      "loss=16.704224494213122\n",
      "proportion=0.9, logistic reg, Training loss=16.704, Testing loss=-208.903\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "degrees = [7]\n",
    "split_ratio = 0.7\n",
    "#split_ratios = [0.7, 0.8 , 0.9]\n",
    "weights = train_test_split_demo(norm_tX, y, split_ratio, seed)\n",
    "\n",
    "\n",
    "#for split_ratio in split_ratios:\n",
    "#      weights = train_test_split_demo(norm_tX, y, split_ratio, seed)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "tX_test[tX_test < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split feature 22 by value as planned\n",
    "tX_test = np.hstack((np.delete(tX_test, 22,axis=1), np.stack([tX_test[:,22] == 0, tX_test[:,22] == 1, tX_test[:,22] == 2, tX_test[:,22] == 3]).T))\n",
    "#print(np.shape(tX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_test_mean = np.nanmean(tX_test,axis=0)\n",
    "#tX_test_std = np.nanstd(tX_test,axis=0)\n",
    "#norm_tX_test = np.subtract(tX_test, tX_test_mean, where=np.isfinite(tX_test_mean))\n",
    "#norm_tX_test = np.divide(norm_tX_test, tX_test_std, where=tX_test_std>0)\n",
    "\n",
    "\n",
    "norm_tX_test = np.subtract(tX_test, tX_mean, where=np.isfinite(tX_mean))\n",
    "norm_tX_test = np.divide(norm_tX_test, tX_std, where=tX_std>0)\n",
    "norm_tX_test[np.isnan(norm_tX_test)] =0\n",
    "norm_tX_test = np.c_[np.ones((y.shape[0], 1)), norm_tX_test]\n",
    "\n",
    "\n",
    "#norm_tX_test = build_poly(norm_tX_test, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.34362703e-02]\n",
      " [ 1.85672892e-04]\n",
      " [-6.38878717e-03]\n",
      " [-2.42917686e-04]\n",
      " [ 3.29591051e-03]\n",
      " [ 1.85172324e-03]\n",
      " [ 1.76455399e-03]\n",
      " [-1.65259864e-03]\n",
      " [ 4.08605078e-04]\n",
      " [-3.83420675e-04]\n",
      " [ 2.56376334e-03]\n",
      " [-3.55705788e-03]\n",
      " [ 4.83848215e-03]\n",
      " [ 1.73910041e-03]\n",
      " [ 4.20129133e-03]\n",
      " [-1.95572528e-05]\n",
      " [-8.29492100e-05]\n",
      " [-6.12787579e-04]\n",
      " [ 2.32570677e-05]\n",
      " [ 7.63452575e-05]\n",
      " [ 2.66634196e-04]\n",
      " [ 1.33690599e-04]\n",
      " [ 2.23752897e-03]\n",
      " [ 1.11447575e-03]\n",
      " [ 3.19649933e-06]\n",
      " [-2.12047832e-06]\n",
      " [-1.76170250e-04]\n",
      " [ 7.28439772e-06]\n",
      " [-3.46256595e-05]\n",
      " [ 2.20377592e-03]\n",
      " [-2.56361289e-03]\n",
      " [ 3.55284733e-04]\n",
      " [ 3.14678671e-03]\n",
      " [-6.01219900e-04]]\n",
      "[[-0.01387732]\n",
      " [ 0.00076037]\n",
      " [-0.0026117 ]\n",
      " ...\n",
      " [ 0.01115793]\n",
      " [ 0.02375724]\n",
      " [-0.01264391]]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = 'data/sample-submission' # TODO: fill in desired name of output file for submission\n",
    "#print(np.shape(y))\n",
    "#print(np.shape(norm_tX_test))\n",
    "#print(np.shape(weights))\n",
    "print(weights)\n",
    "y_pred = predict_labels(weights, norm_tX_test)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
