{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "tX[tX < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import sample_data, standardize\n",
    "seed = 1\n",
    "\n",
    "tX_mean = np.nanmean(tX,axis=0)\n",
    "\n",
    "tX_std = np.nanstd(tX,axis=0)\n",
    "norm_tX = np.subtract(tX, tX_mean, where=np.isfinite(tX_mean))\n",
    "norm_tX = np.divide(norm_tX, tX_std, where=tX_std>0)\n",
    "norm_tX[np.isnan(norm_tX)] =0\n",
    "#norm_tX = np.c_[np.ones(num_samples), norm_tX]\n",
    "\n",
    "#y, norm_tX = sample_data(y, norm_tX, seed, len(y[0]))#size_samples= 100000)\n",
    "norm_tX, mean_x, std_x = standardize(norm_tX)\n",
    "\n",
    "norm_tX = np.c_[np.ones((y.shape[0], 1)), norm_tX]\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "\n",
    "#print(initial_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79513997526, gamma=9e-08, gradient = [[ 2.03666000e+05]\n",
      " [-2.60878135e+03]\n",
      " [ 8.33941282e+04]\n",
      " [ 3.33532744e+03]\n",
      " [-4.56866480e+04]\n",
      " [-4.39684422e+04]\n",
      " [-4.24177513e+04]\n",
      " [ 3.94195876e+04]\n",
      " [-2.90586227e+03]\n",
      " [ 3.62771829e+03]\n",
      " [-3.63630064e+04]\n",
      " [ 4.63680733e+04]\n",
      " [-6.44869325e+04]\n",
      " [-4.12599488e+04]\n",
      " [-5.58221553e+04]\n",
      " [ 2.23834212e+02]\n",
      " [ 1.04472587e+03]\n",
      " [ 7.58118729e+03]\n",
      " [-3.59803838e+02]\n",
      " [-9.78971891e+02]\n",
      " [-5.33114037e+03]\n",
      " [-1.77390454e+03]\n",
      " [-3.21590638e+04]\n",
      " [-3.16913111e+04]\n",
      " [-2.07761451e+04]\n",
      " [-5.84093585e+01]\n",
      " [ 4.19142831e+01]\n",
      " [ 3.02851427e+03]\n",
      " [-1.61418076e+02]\n",
      " [ 8.42878944e+02]\n",
      " [-3.18684808e+04]]\n",
      "Current iteration=100, loss=12086.40984498414, gamma=1e-09, gradient = [[ 1.71098964e+05]\n",
      " [-1.91337345e+03]\n",
      " [ 6.02670888e+04]\n",
      " [ 1.69275375e+03]\n",
      " [-2.11288406e+04]\n",
      " [-2.57547350e+04]\n",
      " [-2.33638437e+04]\n",
      " [ 2.14867000e+04]\n",
      " [-1.41530162e+04]\n",
      " [ 9.12972605e+03]\n",
      " [-1.29102598e+04]\n",
      " [ 3.28292005e+04]\n",
      " [-4.05026176e+04]\n",
      " [-2.56713685e+04]\n",
      " [-3.57577767e+04]\n",
      " [ 2.76532654e+02]\n",
      " [ 8.96350737e+02]\n",
      " [ 6.06322318e+03]\n",
      " [-6.75634155e+01]\n",
      " [-7.77797271e+02]\n",
      " [ 4.64219835e+03]\n",
      " [-1.12557959e+03]\n",
      " [-9.70341394e+03]\n",
      " [-1.14505724e+04]\n",
      " [-4.34668242e+03]\n",
      " [-1.07032245e+01]\n",
      " [-1.35808044e+01]\n",
      " [ 9.14043210e+03]\n",
      " [-2.05916121e+02]\n",
      " [ 4.89129542e+02]\n",
      " [-8.42900710e+03]]\n",
      "Current iteration=200, loss=8052.144342288651, gamma=1e-09, gradient = [[ 1.70270427e+05]\n",
      " [-1.88641310e+03]\n",
      " [ 5.98542083e+04]\n",
      " [ 1.68271739e+03]\n",
      " [-2.07770270e+04]\n",
      " [-2.53891353e+04]\n",
      " [-2.30324547e+04]\n",
      " [ 2.11447693e+04]\n",
      " [-1.42908679e+04]\n",
      " [ 9.16327902e+03]\n",
      " [-1.25845749e+04]\n",
      " [ 3.25701810e+04]\n",
      " [-4.00939339e+04]\n",
      " [-2.53588697e+04]\n",
      " [-3.54176826e+04]\n",
      " [ 2.76755609e+02]\n",
      " [ 8.91940639e+02]\n",
      " [ 6.00085531e+03]\n",
      " [-6.40664008e+01]\n",
      " [-7.72352968e+02]\n",
      " [ 4.75517886e+03]\n",
      " [-1.11355554e+03]\n",
      " [-9.39620602e+03]\n",
      " [-1.11519219e+04]\n",
      " [-4.13366038e+03]\n",
      " [-9.51723375e+00]\n",
      " [-1.53550042e+01]\n",
      " [ 9.17936562e+03]\n",
      " [-2.06775166e+02]\n",
      " [ 4.83009181e+02]\n",
      " [-8.10826583e+03]]\n",
      "Current iteration=300, loss=5496.583422731463, gamma=5e-10, gradient = [[ 1.69747696e+05]\n",
      " [-1.86914483e+03]\n",
      " [ 5.95979107e+04]\n",
      " [ 1.67696024e+03]\n",
      " [-2.05598665e+04]\n",
      " [-2.51602888e+04]\n",
      " [-2.28258319e+04]\n",
      " [ 2.09311772e+04]\n",
      " [-1.43754920e+04]\n",
      " [ 9.18260867e+03]\n",
      " [-1.23837736e+04]\n",
      " [ 3.24089547e+04]\n",
      " [-3.98403271e+04]\n",
      " [-2.51633692e+04]\n",
      " [-3.52060199e+04]\n",
      " [ 2.76871616e+02]\n",
      " [ 8.89165562e+02]\n",
      " [ 5.96161120e+03]\n",
      " [-6.19371664e+01]\n",
      " [-7.68915281e+02]\n",
      " [ 4.82445434e+03]\n",
      " [-1.10606036e+03]\n",
      " [-9.20703636e+03]\n",
      " [-1.09675035e+04]\n",
      " [-4.00258678e+03]\n",
      " [-8.77111411e+00]\n",
      " [-1.64836369e+01]\n",
      " [ 9.20189284e+03]\n",
      " [-2.07312035e+02]\n",
      " [ 4.79201230e+02]\n",
      " [-7.91078664e+03]]\n",
      "Current iteration=400, loss=3514.0307696425443, gamma=5e-10, gradient = [[ 1.69343357e+05]\n",
      " [-1.85565098e+03]\n",
      " [ 5.94018837e+04]\n",
      " [ 1.67280659e+03]\n",
      " [-2.03944089e+04]\n",
      " [-2.49842236e+04]\n",
      " [-2.26672836e+04]\n",
      " [ 2.07670796e+04]\n",
      " [-1.44397280e+04]\n",
      " [ 9.19659527e+03]\n",
      " [-1.22309018e+04]\n",
      " [ 3.22854072e+04]\n",
      " [-3.96463961e+04]\n",
      " [-2.50130200e+04]\n",
      " [-3.50438174e+04]\n",
      " [ 2.76948414e+02]\n",
      " [ 8.87023337e+02]\n",
      " [ 5.93131929e+03]\n",
      " [-6.03305663e+01]\n",
      " [-7.66254790e+02]\n",
      " [ 4.87699676e+03]\n",
      " [-1.10031065e+03]\n",
      " [-9.06314990e+03]\n",
      " [-1.08269552e+04]\n",
      " [-3.90293799e+03]\n",
      " [-8.19523649e+00]\n",
      " [-1.73616228e+01]\n",
      " [ 9.21824878e+03]\n",
      " [-2.07724604e+02]\n",
      " [ 4.76283840e+02]\n",
      " [-7.76059337e+03]]\n",
      "Current iteration=500, loss=1543.5061312556354, gamma=5e-10, gradient = [[ 1.68942531e+05]\n",
      " [-1.84215726e+03]\n",
      " [ 5.92094717e+04]\n",
      " [ 1.66894301e+03]\n",
      " [-2.02325359e+04]\n",
      " [-2.48104987e+04]\n",
      " [-2.25111954e+04]\n",
      " [ 2.06053609e+04]\n",
      " [-1.45023714e+04]\n",
      " [ 9.20963774e+03]\n",
      " [-1.20814453e+04]\n",
      " [ 3.21639348e+04]\n",
      " [-3.94560681e+04]\n",
      " [-2.48647239e+04]\n",
      " [-3.48843199e+04]\n",
      " [ 2.77013525e+02]\n",
      " [ 8.84903785e+02]\n",
      " [ 5.90135217e+03]\n",
      " [-5.87723390e+01]\n",
      " [-7.63616293e+02]\n",
      " [ 4.92819997e+03]\n",
      " [-1.09465190e+03]\n",
      " [-8.92259163e+03]\n",
      " [-1.06894203e+04]\n",
      " [-3.80563305e+03]\n",
      " [-7.62551878e+00]\n",
      " [-1.82362915e+01]\n",
      " [ 9.23355087e+03]\n",
      " [-2.08131267e+02]\n",
      " [ 4.73415817e+02]\n",
      " [-7.61388634e+03]]\n",
      "Current iteration=600, loss=695.9132725131931, gamma=1e-10, gradient = [[ 1.68770454e+05]\n",
      " [-1.83632868e+03]\n",
      " [ 5.91274522e+04]\n",
      " [ 1.66736124e+03]\n",
      " [-2.01636957e+04]\n",
      " [-2.47361639e+04]\n",
      " [-2.24445134e+04]\n",
      " [ 2.05362237e+04]\n",
      " [-1.45289514e+04]\n",
      " [ 9.21498673e+03]\n",
      " [-1.20179169e+04]\n",
      " [ 3.21120921e+04]\n",
      " [-3.93749442e+04]\n",
      " [-2.48012870e+04]\n",
      " [-3.48162401e+04]\n",
      " [ 2.77038129e+02]\n",
      " [ 8.83995150e+02]\n",
      " [ 5.88850685e+03]\n",
      " [-5.81138323e+01]\n",
      " [-7.62483242e+02]\n",
      " [ 4.94991443e+03]\n",
      " [-1.09223505e+03]\n",
      " [-8.86288073e+03]\n",
      " [-1.06309209e+04]\n",
      " [-3.76430814e+03]\n",
      " [-7.38130913e+00]\n",
      " [-1.86131148e+01]\n",
      " [ 9.23984284e+03]\n",
      " [-2.08305140e+02]\n",
      " [ 4.72191870e+02]\n",
      " [-7.55156726e+03]]\n",
      "Current iteration=700, loss=304.2555216173496, gamma=1e-10, gradient = [[ 1.68691009e+05]\n",
      " [-1.83363050e+03]\n",
      " [ 5.90897036e+04]\n",
      " [ 1.66664652e+03]\n",
      " [-2.01320456e+04]\n",
      " [-2.47018946e+04]\n",
      " [-2.24137936e+04]\n",
      " [ 2.05043628e+04]\n",
      " [-1.45411595e+04]\n",
      " [ 9.21740545e+03]\n",
      " [-1.19887155e+04]\n",
      " [ 3.20882192e+04]\n",
      " [-3.93376094e+04]\n",
      " [-2.47720454e+04]\n",
      " [-3.47848884e+04]\n",
      " [ 2.77048809e+02]\n",
      " [ 8.83575915e+02]\n",
      " [ 5.88258032e+03]\n",
      " [-5.78119258e+01]\n",
      " [-7.61960065e+02]\n",
      " [ 4.95988537e+03]\n",
      " [-1.09112176e+03]\n",
      " [-8.83544132e+03]\n",
      " [-1.06040233e+04]\n",
      " [-3.74532007e+03]\n",
      " [-7.26864103e+00]\n",
      " [-1.87873570e+01]\n",
      " [ 9.24269145e+03]\n",
      " [-2.08385269e+02]\n",
      " [ 4.71628277e+02]\n",
      " [-7.52293011e+03]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-d807fdf7b74e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_tX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mlogistic_regression_gd\u001b[1;34m(y, tx, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_gamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mlearning_by_gd\u001b[1;34m(y, tx, w, gamma)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \"\"\"\n\u001b[0;32m     92\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;31m#print('le gradient')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Implementation import logistic_regression_gd\n",
    "\n",
    "# init parameters\n",
    "max_iter = 1000\n",
    "gamma = 0.00000009\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "#print(np.shape(y))\n",
    "#print(np.shape(norm_tX))\n",
    "#print(np.shape(initial_w))\n",
    "\n",
    "    \n",
    "weights = logistic_regression_gd(y, norm_tX, initial_w, max_iter, gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79513997526, gradient = [[ 2.03666000e+05]\n",
      " [-2.60878135e+03]\n",
      " [ 8.33941282e+04]\n",
      " [ 3.33532744e+03]\n",
      " [-4.56866480e+04]\n",
      " [-4.39684422e+04]\n",
      " [-4.24177513e+04]\n",
      " [ 3.94195876e+04]\n",
      " [-2.90586227e+03]\n",
      " [ 3.62771829e+03]\n",
      " [-3.63630064e+04]\n",
      " [ 4.63680733e+04]\n",
      " [-6.44869325e+04]\n",
      " [-4.12599488e+04]\n",
      " [-5.58221553e+04]\n",
      " [ 2.23834212e+02]\n",
      " [ 1.04472587e+03]\n",
      " [ 7.58118729e+03]\n",
      " [-3.59803838e+02]\n",
      " [-9.78971891e+02]\n",
      " [-5.33114037e+03]\n",
      " [-1.77390454e+03]\n",
      " [-3.21590638e+04]\n",
      " [-3.16913111e+04]\n",
      " [-2.07761451e+04]\n",
      " [-5.84093585e+01]\n",
      " [ 4.19142831e+01]\n",
      " [ 3.02851427e+03]\n",
      " [-1.61418076e+02]\n",
      " [ 8.42878944e+02]\n",
      " [-3.18684808e+04]]\n",
      "Current iteration=100, loss=-175304.56711818115, gradient = [[ 1.39677497e+05]\n",
      " [-5.83959913e+02]\n",
      " [ 5.04079337e+04]\n",
      " [ 1.86045022e+03]\n",
      " [-1.33748162e+04]\n",
      " [-1.40708371e+04]\n",
      " [-1.33839710e+04]\n",
      " [ 1.09985920e+04]\n",
      " [-1.71244524e+04]\n",
      " [ 8.29453934e+03]\n",
      " [-5.86010883e+03]\n",
      " [ 2.60617448e+04]\n",
      " [-3.05210295e+04]\n",
      " [-1.59503414e+04]\n",
      " [-2.61824991e+04]\n",
      " [ 2.57605357e+02]\n",
      " [ 7.55496040e+02]\n",
      " [ 4.14946956e+03]\n",
      " [-2.06320657e+01]\n",
      " [-5.72810270e+02]\n",
      " [ 6.91541456e+03]\n",
      " [-7.83924761e+02]\n",
      " [-3.36636228e+03]\n",
      " [-4.75908745e+03]\n",
      " [ 5.21536169e+01]\n",
      " [ 2.65636465e+01]\n",
      " [-9.01669770e+01]\n",
      " [ 8.27688741e+03]\n",
      " [-2.29898227e+02]\n",
      " [ 3.21097703e+02]\n",
      " [-1.86505032e+03]]\n",
      "Current iteration=200, loss=-380770.6513531717, gradient = [[ 1.23389009e+05]\n",
      " [ 3.24120543e+02]\n",
      " [ 5.06004615e+04]\n",
      " [ 2.14888044e+03]\n",
      " [-1.31416694e+04]\n",
      " [-9.86507919e+03]\n",
      " [-9.91132837e+03]\n",
      " [ 7.42864559e+03]\n",
      " [-1.79438052e+04]\n",
      " [ 6.60070681e+03]\n",
      " [-5.50321890e+03]\n",
      " [ 2.51265440e+04]\n",
      " [-2.99870315e+04]\n",
      " [-1.27529545e+04]\n",
      " [-2.32686718e+04]\n",
      " [ 2.28475885e+02]\n",
      " [ 7.07911947e+02]\n",
      " [ 3.79533892e+03]\n",
      " [-5.46293832e+01]\n",
      " [-4.69290778e+02]\n",
      " [ 7.24951750e+03]\n",
      " [-6.94762838e+02]\n",
      " [-3.42303344e+03]\n",
      " [-4.35357020e+03]\n",
      " [ 1.67137289e+02]\n",
      " [ 3.18838783e+01]\n",
      " [-1.13909869e+02]\n",
      " [ 6.52613539e+03]\n",
      " [-2.16476192e+02]\n",
      " [ 2.71653171e+02]\n",
      " [-2.03029609e+03]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5c5bbd7a5c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregu_logistic_regression_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_tX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mregu_logistic_regression_gd\u001b[1;34m(y, tx, lambda_, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_penalized_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;31m# log info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mlearning_by_penalized_gd\u001b[1;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;31m#print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[1;31m#loss, gradient = penalized_lr(y, tx, w, lambda_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Implementation import regu_logistic_regression_gd\n",
    "\n",
    "# init parameters\n",
    "max_iter = 1000\n",
    "gamma = 0.00000009\n",
    "lambda_ = 0\n",
    "\n",
    "\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "\n",
    "weights = regu_logistic_regression_gd(y, norm_tX, lambda_, initial_w, max_iter, gamma )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares and Polynomial regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    \n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import compute_mse\n",
    "from plots import *\n",
    "\n",
    "def polynomial_regression(x):\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1,3,7,12]\n",
    "      \n",
    "    for ind, degree in enumerate(degrees):\n",
    "        # form dataset to do polynomial regression.\n",
    "        tx = build_poly(x, degree)\n",
    "        \n",
    "        # least squares\n",
    "        weights = least_squares(y, tx)\n",
    "             \n",
    "        # compute RMSE\n",
    "        rmse = np.sqrt(2 * compute_mse(y, tx, weights))\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "       \n",
    "        # print\n",
    "        #print(np.shape(y))\n",
    "        #print(np.shape(tx))\n",
    "        #print(np.shape(weights))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = polynomial_regression(norm_tX)\n",
    "#weights = polynomial_regression(norm_tX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    weight = least_squares(y_tr, tx_tr)\n",
    "    \n",
    "    #if ((ratio == 0,9) and (degree == 7)):\n",
    "    weights = weight\n",
    "    #print(weights)\n",
    "\n",
    "    # calculate RMSE for train and test data.\n",
    "    rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo_2(x, y, degree, ratio, seed, w):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    weight = w\n",
    "    \n",
    "    # calculate RMSE for train and test data.\n",
    "    rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    return weight\n",
    "    \n",
    "    #if ((ratio == 0,9) and (degree == 7)):\n",
    "    \n",
    "    #print(weights)\n",
    "\n",
    "    # calculate RMSE for train and test data.\n",
    "    rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6\n",
    "degrees = [7]\n",
    "split_ratios = [0.9]\n",
    "#weights = train_test_split_demo_2(norm_tX, y, degree, split_ratio, seed, weights)\n",
    "#weights = train_test_split_demo(norm_tX, y, degree, split_ratio, seed)\n",
    "#print(weights)\n",
    "\n",
    "#for split_ratio in split_ratios:\n",
    "    #for degree in degrees:\n",
    "      #  train_test_split_demo(norm_tX, y, degree, split_ratio, seed)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "#tX_test[tX_test < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_mean = np.nanmean(tX_test,axis=0)\n",
    "tX_test_std = np.nanstd(tX_test,axis=0)\n",
    "norm_tX_test = np.subtract(tX_test, tX_test_mean, where=np.isfinite(tX_test_mean))\n",
    "norm_tX_test = np.divide(norm_tX_test, tX_test_std, where=tX_test_std>0)\n",
    "norm_tX_test = np.c_[np.ones((y.shape[0], 1)), norm_tX_test]\n",
    "#norm_tX_test = build_poly(norm_tX_test, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.18512471e-01]\n",
      " [ 7.54498170e-03]\n",
      " [-2.31343383e-01]\n",
      " [-7.36392543e-03]\n",
      " [ 1.00090770e-01]\n",
      " [ 1.10259604e-01]\n",
      " [ 1.02141798e-01]\n",
      " [-9.52617251e-02]\n",
      " [ 3.43406399e-02]\n",
      " [-2.48680395e-02]\n",
      " [ 7.10424253e-02]\n",
      " [-1.27639651e-01]\n",
      " [ 1.65819248e-01]\n",
      " [ 1.06507895e-01]\n",
      " [ 1.44645602e-01]\n",
      " [-8.68444579e-04]\n",
      " [-3.22895969e-03]\n",
      " [-2.31962345e-02]\n",
      " [ 5.67026833e-04]\n",
      " [ 2.89662835e-03]\n",
      " [-4.37065458e-03]\n",
      " [ 4.60402877e-03]\n",
      " [ 5.90859737e-02]\n",
      " [ 6.28703060e-02]\n",
      " [ 3.36388848e-02]\n",
      " [ 1.03863673e-04]\n",
      " [-4.23042045e-05]\n",
      " [-2.42682110e-02]\n",
      " [ 6.29620467e-04]\n",
      " [-2.08265797e-03]\n",
      " [ 5.60120526e-02]]\n",
      "[[-1.62337415]\n",
      " [-1.14891405]\n",
      " [-1.15359958]\n",
      " ...\n",
      " [-0.68263614]\n",
      " [-0.30230101]\n",
      " [-1.5155485 ]]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = 'data/sample-submission' # TODO: fill in desired name of output file for submission\n",
    "#print(np.shape(y))\n",
    "#print(np.shape(norm_tX_test))\n",
    "#print(np.shape(weights))\n",
    "print(weights)\n",
    "y_pred = predict_labels(weights, norm_tX_test)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
