{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "tX[tX < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 33)\n"
     ]
    }
   ],
   "source": [
    "# split feature 22 by value as planned\n",
    "tX = np.hstack((np.delete(tX, 22,axis=1), np.stack([tX[:,22] == 0, tX[:,22] == 1, tX[:,22] == 2, tX[:,22] == 3]).T))\n",
    "print(np.shape(tX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 33)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import sample_data\n",
    "seed = 1\n",
    "\n",
    "tX_mean = np.nanmean(tX,axis=0)\n",
    "\n",
    "tX_std = np.nanstd(tX,axis=0)\n",
    "norm_tX = np.subtract(tX, tX_mean, where=np.isfinite(tX_mean))\n",
    "norm_tX = np.divide(norm_tX, tX_std, where=tX_std>0)\n",
    "norm_tX[np.isnan(norm_tX)] =0\n",
    "\n",
    "\n",
    "\n",
    "#norm_tX = np.c_[np.ones((y.shape[0], 1)), norm_tX]\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "print(np.shape(norm_tX))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237360, 33)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outliers: we remove any datapoint that is more than 4 standard derivations removed from the mean\n",
    "# since normalisation was applied, this translate to\n",
    "norm_tX = np.delete(norm_tX, np.linalg.norm(norm_tX, ord=np.inf, axis=1) > 4, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 34)\n",
      "(34, 1)\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "Current iteration=0, loss=173286.79513997794, gamma=1e-05\n",
      "Current iteration=100, loss=173272.9512896622, gamma=1e-05\n",
      "Current iteration=200, loss=173259.22002903727, gamma=1e-05\n",
      "Current iteration=300, loss=173245.60055404122, gamma=1e-05\n",
      "Current iteration=400, loss=173232.09206760937, gamma=1e-05\n",
      "Current iteration=500, loss=173218.69377961237, gamma=1e-05\n",
      "Current iteration=600, loss=173205.40490678584, gamma=1e-05\n",
      "Current iteration=700, loss=173192.2246726617, gamma=1e-05\n",
      "Current iteration=800, loss=173179.15230749932, gamma=1e-05\n",
      "Current iteration=900, loss=173166.1870482183, gamma=1e-05\n",
      "Current iteration=1000, loss=173153.32813833142, gamma=1e-05\n",
      "Current iteration=1100, loss=173140.5748278781, gamma=1e-05\n",
      "Current iteration=1200, loss=173127.9263733594, gamma=1e-05\n",
      "Current iteration=1300, loss=173115.38203767297, gamma=1e-05\n",
      "Current iteration=1400, loss=173102.9410900489, gamma=1e-05\n",
      "loss=173090.60280598645\n"
     ]
    }
   ],
   "source": [
    "from Implementation import logistic_regression_gd\n",
    "\n",
    "# init parameters\n",
    "max_iter = 1500\n",
    "gamma = 0.00001\n",
    "norm_tX = np.c_[np.ones((y.shape[0], 1)), norm_tX]\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "y[y==-1] = 0\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(norm_tX))\n",
    "print(np.shape(initial_w))\n",
    "print(y)\n",
    "    \n",
    "weights, loss_lr = logistic_regression_gd(y, norm_tX, initial_w, max_iter, gamma)\n",
    "\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79513997794\n",
      "Current iteration=100, loss=134255.5107390551\n",
      "Current iteration=200, loss=132938.5845122488\n",
      "Current iteration=300, loss=132605.87844800614\n",
      "Current iteration=400, loss=132474.29865345795\n",
      "Current iteration=500, loss=132402.6074748882\n",
      "Current iteration=600, loss=132355.89267910525\n",
      "Current iteration=700, loss=132322.38299321564\n",
      "Current iteration=800, loss=132296.9784879584\n",
      "Current iteration=900, loss=132277.07564612041\n",
      "Current iteration=1000, loss=132261.17952086788\n",
      "Current iteration=1100, loss=132248.34256149895\n",
      "Current iteration=1200, loss=132237.91188336047\n",
      "Current iteration=1300, loss=132229.4078330733\n",
      "Current iteration=1400, loss=132222.46210773868\n",
      "Current iteration=1500, loss=132216.78391975214\n",
      "loss=132211.14764576976\n"
     ]
    }
   ],
   "source": [
    "from Implementation import regu_logistic_regression_gd\n",
    "\n",
    "# init parameters\n",
    "max_iter = 1600\n",
    "gamma = 0.00001\n",
    "#gamma = 0.00000009\n",
    "lambda_ = 0.1\n",
    "norm_tX = np.c_[np.ones((y.shape[0], 1)), norm_tX]\n",
    "\n",
    "\n",
    "initial_w = np.zeros((norm_tX.shape[1], 1))\n",
    "\n",
    "\n",
    "weights, loss_rlr = regu_logistic_regression_gd(y, norm_tX, lambda_, initial_w, max_iter, gamma )\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares and Polynomial regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = polynomial_regression(norm_tX)\n",
    "#weights = polynomial_regression(norm_tX2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression with a polynome 7 degree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Implementation import split_data, build_poly, ridge_regression, compute_mse\n",
    "\n",
    "def ridge_regression_split(x, y), lambda:\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "   \n",
    "    # define parameter\n",
    "    degree = 7\n",
    "    seed = 29\n",
    "    ratio = 0.8\n",
    "    lambda_ = 0.1\n",
    "     \n",
    "    # split data\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "     \n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    \n",
    "    # ridge regression with different lambda\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    # ridge regression\n",
    "    \n",
    "    weight, loss_rmse = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2 * compute_mse(y_tr, tx_tr, weight)))\n",
    "    rmse_te.append(np.sqrt(2 * compute_mse(y_te, tx_te, weight)))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           p=ratio, d=degree, l=lambda_, tr=rmse_tr[0], te=rmse_te[0]))\n",
    "    \n",
    "    return weight, loss_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.8, degree=7, lambda=0.100, Training RMSE=0.777, Testing RMSE=0.893\n",
      "[-4.05284362e-03  1.10836875e-01 -1.64612064e-01  1.84564099e-02\n",
      "  3.83275656e-02  1.40727829e-02  1.56753986e-02 -2.42532676e-02\n",
      "  5.13592051e-02 -2.83875210e-02  4.42799676e-02 -8.31254786e-02\n",
      "  2.23265309e-02  3.19802415e-02  1.17954743e-01 -1.45474120e-03\n",
      " -3.52183970e-04  2.57893504e-02  1.34224232e-03  1.06619805e-03\n",
      " -4.03257755e-03  1.24685716e-03  1.24447543e-02  2.44537568e-02\n",
      " -1.84593729e-03 -7.97776361e-04  1.55485982e-02 -1.77448347e-04\n",
      " -7.77658991e-04  1.94949894e-02 -3.76654956e-03  1.47909317e-03\n",
      "  1.99811822e-03  1.26394275e-03 -1.27467973e-01  1.87252879e-02\n",
      " -8.90480247e-02  3.57948982e-02  2.30983421e-02  1.83105095e-02\n",
      "  4.64451312e-03  4.55265768e-03  4.61305795e-03 -2.39421523e-02\n",
      "  5.74505332e-02  9.12288283e-03  5.63885272e-03 -4.14755690e-02\n",
      " -7.78831042e-03 -3.90538050e-03 -8.20514121e-03 -2.11755877e-02\n",
      " -2.54387975e-03  5.47019659e-02 -9.61990983e-04 -3.67408823e-02\n",
      " -2.79458383e-02  3.67944942e-02 -4.68302763e-04 -5.67461762e-03\n",
      "  2.20758494e-02 -2.46759966e-03 -1.45639882e-02 -5.59610901e-03\n",
      " -2.83890544e-03 -1.07923765e-03 -3.94655275e-04 -1.76070394e-02\n",
      " -1.93821293e-02  9.41225999e-03  3.41253151e-03 -1.17646178e-03\n",
      " -1.20738384e-04 -1.54325708e-02  1.73405747e-02  4.44048788e-03\n",
      "  1.79679513e-02 -3.03688718e-02  2.07612168e-02  2.08227346e-02\n",
      "  5.51708117e-03 -6.08809349e-04  2.51581596e-04  1.94039505e-02\n",
      "  2.07142388e-04  1.35230492e-03 -2.79919111e-02  6.52948574e-04\n",
      "  8.73885273e-03  1.48281895e-02 -1.14703882e-03 -3.49322833e-04\n",
      "  6.53454531e-03 -1.64980584e-04 -3.75506687e-04 -9.30716439e-03\n",
      " -6.05944839e-03 -8.50917458e-04  3.91965511e-04  1.21643508e-04\n",
      "  1.65534395e-02  1.19101536e-02  1.53879834e-03  2.85855373e-03\n",
      "  1.74957239e-02  1.88931359e-03 -2.47385990e-03 -2.57739899e-02\n",
      " -9.03712005e-04 -5.50263199e-03  6.50327969e-03  1.21381998e-02\n",
      "  5.24791191e-03 -4.45197869e-04 -1.13854419e-02 -3.33049414e-03\n",
      " -6.40713926e-03 -2.23502277e-02 -2.05381421e-03  5.70435873e-03\n",
      "  9.87522057e-04 -3.84961462e-03 -9.16963073e-03  3.98012762e-02\n",
      " -3.94615924e-04 -1.35974892e-03  2.60288680e-02 -1.75696477e-03\n",
      "  3.22935383e-03 -8.07884511e-03 -3.53730424e-03 -4.95936572e-04\n",
      " -4.25229957e-05 -2.67773828e-03 -2.05323061e-03 -2.68423006e-04\n",
      " -1.92964679e-03 -1.69803150e-02 -1.29699425e-03  1.96900522e-03\n",
      "  3.53301760e-03  6.65316456e-05  8.74750512e-04 -6.33006639e-04\n",
      "  1.48828070e-02  1.39607662e-02  2.76187188e-05  7.82894387e-04\n",
      "  6.83519372e-04  8.08973823e-04 -1.03788877e-04  1.24267704e-03\n",
      " -5.36680302e-04  1.04569054e-03  9.45995212e-04  2.42159357e-03\n",
      "  1.91372355e-03  1.92914885e-04  8.76456607e-05  7.25434398e-04\n",
      " -2.16532944e-04 -3.87326718e-04 -9.36958352e-03 -3.75411008e-03\n",
      " -3.46006835e-04 -1.17354209e-06  1.66342063e-04  1.35317603e-04\n",
      "  1.29247628e-05  2.88014599e-04  5.75622089e-03  2.07548454e-04\n",
      "  1.86910123e-04  4.02874427e-03 -2.18868352e-06 -7.26056462e-05\n",
      "  2.80542760e-05  1.06994546e-02  2.74882476e-03 -1.09557790e-06\n",
      "  7.40189747e-04  1.55642833e-03 -4.39483857e-05  3.15979794e-03\n",
      "  7.39053728e-04  2.31209242e-05 -5.59229635e-04 -8.98016384e-05\n",
      " -2.58579195e-04 -5.72959476e-03 -1.61715692e-04 -1.25581714e-06\n",
      " -3.48945191e-03  4.93510269e-04  2.21982002e-05 -1.19178371e-02\n",
      " -6.61846784e-03 -1.01084643e-03 -4.65103592e-05 -3.57354470e-06\n",
      " -3.04091773e-06 -2.00870607e-07 -1.29706289e-05 -1.01287006e-03\n",
      " -9.82082098e-06 -8.13875479e-05 -8.82852571e-04  2.78994817e-08\n",
      "  2.38014200e-06 -4.59291752e-07  2.22908008e-03  5.69969121e-03\n",
      "  1.77231849e-08 -1.33902716e-04 -2.37903767e-04  8.61733025e-07\n",
      " -5.40559658e-05 -8.22338082e-04 -3.66037907e-07 -5.39489807e-04\n",
      "  2.67501361e-06  9.58168000e-06 -2.36384251e-04  2.97544103e-04\n",
      " -2.47516730e-08 -8.10659323e-05  4.10279745e-04 -5.07634188e-07\n",
      " -1.42526346e-02 -9.18625008e-03 -1.85056721e-03 -1.32680251e-04]\n",
      "(232,)\n"
     ]
    }
   ],
   "source": [
    "weights = ridge_regression_split(norm_tX, y)\n",
    "#print(weights)\n",
    "#print(np.shape(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data Log Res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n",
      "(200000, 34)\n",
      "(34, 1)\n",
      "Current iteration=0, loss=138629.4361119834, gamma=9e-08\n",
      "Current iteration=100, loss=138629.33514666208, gamma=9e-08\n",
      "Current iteration=200, loss=138629.23418874817, gamma=9e-08\n",
      "Current iteration=300, loss=138629.13323824687, gamma=9e-08\n",
      "Current iteration=400, loss=138629.0322951576, gamma=9e-08\n",
      "Current iteration=500, loss=138628.93135948002, gamma=9e-08\n",
      "Current iteration=600, loss=138628.8304312135, gamma=9e-08\n",
      "Current iteration=700, loss=138628.7295103576, gamma=9e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-5b97dfe54339>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msplit_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#split_ratios = [0.7, 0.8 , 0.9]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split_demo_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_tX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mtrain_test_split_demo_lr\u001b[1;34m(x, y, ratio, seed)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0my_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mlogistic_regression_gd\u001b[1;34m(y, tx, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;31m#gamma = update_gamma(gamma,loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mlearning_by_gd\u001b[1;34m(y, tx, w, gamma)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \"\"\"\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m     \u001b[1;31m#grad = calculate_gradient(y, tx, w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_stoch_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\MASTER\\Ma1\\Machine Learning\\project1\\ML_Project1\\Implementation.py\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;34m\"\"\"apply sigmoid function on t.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;31m#return 1.0 / (1 + np.exp(-t))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogaddexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Implementation import train_test_split_demo_lr\n",
    "seed = 6\n",
    "degrees = [7]\n",
    "split_ratio = 0.8\n",
    "#split_ratios = [0.7, 0.8 , 0.9]\n",
    "weights = train_test_split_demo_lr(norm_tX, y, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv.zip' # TODO: download train data and supply path here \n",
    "y, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that very low values are used to signal unavailable data\n",
    "tX_test[tX_test < -900] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 33)\n"
     ]
    }
   ],
   "source": [
    "# split feature 22 by value as planned\n",
    "tX_test = np.hstack((np.delete(tX_test, 22,axis=1), np.stack([tX_test[:,22] == 0, tX_test[:,22] == 1, tX_test[:,22] == 2, tX_test[:,22] == 3]).T))\n",
    "print(np.shape(tX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 232)\n",
      "(232,)\n",
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "norm_tX_test = np.subtract(tX_test, tX_mean, where=np.isfinite(tX_mean))\n",
    "norm_tX_test = np.divide(norm_tX_test, tX_std, where=tX_std>0)\n",
    "norm_tX_test[np.isnan(norm_tX_test)] =0\n",
    "#norm_tX_test = np.c_[np.ones((y.shape[0], 1)), norm_tX_test]\n",
    "norm_tX_test = build_poly(norm_tX_test, 7)\n",
    "\n",
    "print(np.shape(norm_tX_test))\n",
    "print(np.shape(weights))\n",
    "print(np.shape(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.18598128 -0.57506826 -0.18612134 ...  0.02961414  0.08455748\n",
      " -0.69189037]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = 'data/sample-submission' # TODO: fill in desired name of output file for submission\n",
    "#print(np.shape(y))\n",
    "#print(np.shape(norm_tX_test))\n",
    "#print(np.shape(weights))\n",
    "#print(weights)\n",
    "y_pred = predict_labels(weights, norm_tX_test)\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
